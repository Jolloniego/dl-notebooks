{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clase7 Solucion.ipynb","provenance":[],"collapsed_sections":["zBfOKG3iMt9Q"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7VMtkNDwxZ0m"},"source":["# Datos\n","\n","Vamos a usar el dataset de IMDB para clasificación de reseñas de películas, el objetivo del mismo es detectar si una reseña tiene sentimiento **positivo** o **negativo**.\n","\n","Descarguen el dataset de este [link](https://drive.google.com/file/d/1i0bBI4p80AxsLgnWcXkxVT65AahIzePu/view?usp=sharing) y subanlo a una carpeta **data** en la raiz de su drive personal.\n"]},{"cell_type":"code","metadata":{"id":"zh7A9mjs3cfU","executionInfo":{"status":"ok","timestamp":1604019126293,"user_tz":180,"elapsed":24121,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"eb2492a3-95f2-4f3d-f97b-ca30965b8e47","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","! cp \"/content/drive/My Drive/data/IMDB_Dataset.zip\" .\n","! unzip -q IMDB_Dataset.zip\n","! rm IMDB_Dataset.zip\n","! ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"," drive\t'IMDB Dataset.csv'   sample_data   word2vec.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BBaFc6ZZzp2T","executionInfo":{"status":"ok","timestamp":1602442484277,"user_tz":180,"elapsed":25120,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"9f21a0e7-9c16-4dfd-dcbc-ce21ffb7b916","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["imdb_data = pd.read_csv(\"IMDB Dataset.csv\")\n","\n","#sentiment count\n","print(imdb_data.columns)\n","imdb_data['sentiment'].value_counts()\n","\n","# Convert positive and negative into binary classes (1-0)\n","from sklearn.preprocessing import LabelBinarizer\n","lb = LabelBinarizer()\n","\n","sentiment_data = lb.fit_transform(imdb_data[\"sentiment\"])\n","imdb_data['sentiment'] = sentiment_data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['review', 'sentiment'], dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rQFZbEtC1T94","executionInfo":{"status":"ok","timestamp":1602442484278,"user_tz":180,"elapsed":25113,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"a73d54fe-250d-4b57-b94e-bdb6f3204564","colab":{"base_uri":"https://localhost:8080/","height":359}},"source":["imdb_data.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Probably my all-time favorite movie, a story o...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I sure would like to see a resurrection of a u...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This show was an amazing, fresh &amp; innovative i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Encouraged by the positive comments about this...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>If you like original gut wrenching laughter yo...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  One of the other reviewers has mentioned that ...          1\n","1  A wonderful little production. <br /><br />The...          1\n","2  I thought this was a wonderful way to spend ti...          1\n","3  Basically there's a family where a little boy ...          0\n","4  Petter Mattei's \"Love in the Time of Money\" is...          1\n","5  Probably my all-time favorite movie, a story o...          1\n","6  I sure would like to see a resurrection of a u...          1\n","7  This show was an amazing, fresh & innovative i...          0\n","8  Encouraged by the positive comments about this...          0\n","9  If you like original gut wrenching laughter yo...          1"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"rhdlJtOo2YC-"},"source":["# Imports\n"]},{"cell_type":"code","metadata":{"id":"YaxC7dfq2aKT","executionInfo":{"status":"ok","timestamp":1604019129647,"user_tz":180,"elapsed":2479,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"7c1900e2-ed4d-4495-925b-02dff1d39a05","colab":{"base_uri":"https://localhost:8080/"}},"source":["import re\n","from bs4 import BeautifulSoup\n","\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Lkr7Vz950IeY"},"source":["# Preprocesamiento Inicial\n","\n","Como toda tarea de NLP tenemos que comenzar preprocesando los datos, eliminando palabras que no nos sirve, caracteres especiales, etc.\n","\n","En particular hay tres tareas a ser realizadas basadas en un análisis inicial del dataset (mirando ejemplos al azar del mismo)\n","\n","\n","\n","1.   Eliminar tags html (vamos a utilizar BeautifulSoup para esto)\n","2.   Eliminar texto entre parentesis rectos (Usando la siguiente expresion regular: ```\\[[^]]*\\]``` )\n","3. Eliminar caracteres especiales, usando una regex quitar todos los caracteres que no son ni letras ni números (```[^a-zA-z0-9\\s] ``` )\n","\n"]},{"cell_type":"code","metadata":{"id":"SIiaBeGOwIOQ"},"source":["def strip_html(text):\n","  soup = BeautifulSoup(text, \"html.parser\")\n","  return soup.get_text()\n","\n","\n","def remove_between_square_brackets(text):\n","  return re.sub('\\[[^]]*\\]', '', text)\n","\n","\n","def remove_special_characters(text):\n","  pattern = r'[^a-zA-z0-9\\s]'\n","  text = re.sub(pattern,'',text)\n","  return text\n","\n","\n","def low_level_preproc(text):\n","  text = strip_html(text)\n","  text = remove_between_square_brackets(text)\n","  text = remove_special_characters(text)\n","  return text\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].apply(low_level_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGJUuc0O16iv"},"source":["# Preprocesamiento de alto nivel\n","\n","Una vez tenemos el texto limpio y trabajable volvemos a hacer otro pasaje de preprocesamiento de más alto nivel, ahora vamos a querer:\n","\n","\n","\n","1.   Transformar todo el texto a minúscula\n","2.   Quitar stop words (usando nltk)\n","3.   Lemmatizar usando nltk WordNetLemmatizer\n","\n","Para todo esto vamos a necesitar trabajar con **tokens** palabras individuales, en este caso vamos a separar por **whitespace**, pero se podrían usar mejores estrategias.\n","\n"]},{"cell_type":"code","metadata":{"id":"KeKNTxvzwISR"},"source":["all_stopwords = set(stopwords.words(\"english\"))\n","\n","def remove_stop_words(full_text_line):\n","  tokens = full_text_line.split()\n","  tokens = [tok for tok in tokens if tok not in all_stopwords]\n","\n","  return \" \".join(tokens)\n","\n","\n","def lemmatize(text):\n","  wnl= WordNetLemmatizer()\n","  lemas = [wnl.lemmatize(word) for word in text.split()]\n","\n","  return \" \".join(lemas)\n","\n","\n","def high_level_preproc(text):\n","  text = remove_stop_words(text)\n","  return lemmatize(text)\n","\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].str.lower()\n","imdb_data['review'] = imdb_data['review'].apply(high_level_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9k3wz_XUql1d","executionInfo":{"status":"ok","timestamp":1602442514005,"user_tz":180,"elapsed":54817,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"eb5fdfb5-7820-4674-cc22-a48b026b463f","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["imdb_data['review'].head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    one reviewer mentioned watching 1 oz episode y...\n","1    wonderful little production filming technique ...\n","2    thought wonderful way spend time hot summer we...\n","3    basically there family little boy jake think t...\n","4    petter matteis love time money visually stunni...\n","5    probably alltime favorite movie story selfless...\n","6    sure would like see resurrection dated seahunt...\n","7    show amazing fresh innovative idea 70 first ai...\n","8    encouraged positive comment film looking forwa...\n","9    like original gut wrenching laughter like movi...\n","Name: review, dtype: object"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"MAynpGKc3dYn"},"source":["# Modelando\n","\n","Para modelar vamos a comenzar separando el dataset."]},{"cell_type":"code","metadata":{"id":"OFZn34Q63lLE","executionInfo":{"status":"ok","timestamp":1602442514006,"user_tz":180,"elapsed":54810,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"a3ea6806-c127-482c-8654-228bf4fef478","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#split the dataset  \n","#train dataset\n","train_reviews = imdb_data.review[:40000]\n","train_sentiments = imdb_data.sentiment[:40000]\n","\n","#test dataset\n","test_reviews = imdb_data.review[40000:]\n","test_sentiments = imdb_data.sentiment[40000:]\n","\n","\n","print(\"Train set:\", train_reviews.shape, train_sentiments.shape)\n","print(\"Test set:\", test_reviews.shape, test_sentiments.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train set: (40000,) (40000,)\n","Test set: (10000,) (10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U16zBbGzAb5z"},"source":["Vamos a generar vectores para las reseñas usando TF-IDF (sklearn). Vamos a hacer uso del parametro ```max_features``` que nos permite controlar cuántas palabras considerar para generar los vectores (en orden de frecuencia). Luego usamos esa representacion vectorial para entrenar y testear un regresor logístico (LogisticRegressor).\n","\n","\n","Vamos a entrenar el modelo por 500 iteraciones como máximo y usamos l2 como regularizador."]},{"cell_type":"code","metadata":{"id":"9RVouRZ_ql3I"},"source":["vectorizer = TfidfVectorizer(max_features=300)\n","\n","train_vectors = vectorizer.fit_transform(train_reviews)\n","\n","test_vectors = vectorizer.transform(test_reviews)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCOsCtrEAaJj","executionInfo":{"status":"ok","timestamp":1602442518217,"user_tz":180,"elapsed":59011,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"af85c28a-4c53-4629-f60e-754caacf3e9f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(penalty='l2', max_iter=500, random_state=42)\n","clf.fit(train_vectors, train_sentiments)\n","\n","predictions = clf.predict(test_vectors)\n","\n","accuracy_score(test_sentiments, predictions)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8112"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"btnSQOb2JwIL"},"source":["# Vectores pre entrenados\n","Ahora vamos a ver si podemos superar la performance del modelo haciendo uso de deep learning.  Primero vamos a entrenar el mismo modelo usando los embeddings preentrenados de Word2Vec (usando gensim). \n","\n","Luego vamos a darle esos embeddings a un MLP y ver si logramos superar la performance anterior."]},{"cell_type":"code","metadata":{"id":"qYqJ0uJ1XH6Y","executionInfo":{"status":"ok","timestamp":1604019131854,"user_tz":180,"elapsed":836,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}}},"source":["import gensim"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7oiP209Jvs1","executionInfo":{"status":"ok","timestamp":1604019143750,"user_tz":180,"elapsed":11067,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}}},"source":["w2v = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec.txt\", binary=False)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ap22Pw93U586"},"source":["mean_vector = np.mean(w2v.vectors, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVwCEMusJvw0"},"source":["def get_sentence_embedding(text):\n","  all_embeddings = [w2v[word] if word in w2v else mean_vector for word in text.split()]\n","  \n","  # Can use mean or max\n","  return np.mean(all_embeddings, axis=0)\n","\n","\n","train_vectors = [get_sentence_embedding(sent) for sent in train_reviews]\n","test_vectors = [get_sentence_embedding(sent) for sent in test_reviews]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"884gkcqLVvyL","executionInfo":{"status":"ok","timestamp":1602442541565,"user_tz":180,"elapsed":82343,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"8cab32ff-f779-4df9-c991-9cbd4af136be","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["clf = LogisticRegression(penalty='l2', max_iter=500, random_state=42)\n","clf.fit(train_vectors, train_sentiments)\n","\n","predictions = clf.predict(test_vectors)\n","\n","accuracy_score(test_sentiments, predictions)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8363"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"osIQT4OhVv0g"},"source":["\"\"\"\n","Preguntas:\n","\n","1) Que dimension tienen los vectores de word2vec? Como se compara con los usados anteriormente? \n","1.5) Qué pasa si aumentamos el número de features para los modelos anteriores? \n","2) Cual es la opción más eficiente?\n","3) Se observan diferencias al usar mean/max para crear los vectores de documento?\n","4) Que estrategias se pueden probar para los vectores de palabras que no estan en el vocabulario? \n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-2GvqL87Yipe"},"source":["# Deep Learning\n"]},{"cell_type":"markdown","metadata":{"id":"5oxjMLJAW8qr"},"source":["MLP: vamos a crear un MLP para atacar ese mismo problema, el diseño corre por su cuenta pero deberían ser capaces de obetener mejor performance en test que los modelos anteriores.\n"]},{"cell_type":"code","metadata":{"id":"SqH91xLsYlQr","executionInfo":{"status":"ok","timestamp":1602442545190,"user_tz":180,"elapsed":85959,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"345ab2e9-84eb-4308-931f-adb27aa8666a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Imports\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EUuaAbANZscl"},"source":["def train_epoch(training_model, loader, criterion, optim):\n","    training_model.train()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    for data, labels in loader:\n","      all_labels.extend(labels.numpy())  \n","\n","      optim.zero_grad()\n","\n","      predictions = training_model(data.to(DEVICE))\n","      all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","      loss = criterion(predictions, labels.to(DEVICE))\n","      \n","      loss.backward()\n","      optim.step()\n","\n","      epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","\n","\n","def validation_epoch(val_model, loader, criterion):\n","    val_model.eval()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    with torch.no_grad():\n","      for data, labels in loader:\n","        all_labels.extend(labels.numpy())  \n","\n","        predictions = val_model(data.to(DEVICE))\n","        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","        loss = criterion(predictions, labels.to(DEVICE))\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","  \n","\n","def train_model(model, train_loader, test_loader, criterion, optim, number_epochs):\n","  train_history = []\n","  test_history = []\n","  accuracy_history = []\n","\n","  for epoch in range(number_epochs):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n","      train_history.append(train_loss)\n","      print(\"Training epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, train_loss, train_acc, time.time() - start_time))\n","\n","      start_time = time.time()\n","      test_loss, acc = validation_epoch(model, test_loader, criterion)\n","      test_history.append(test_loss)\n","      accuracy_history.append(acc)\n","      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, test_loss, acc, time.time() - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M46rdl18JvzG"},"source":["class MLP(nn.Module):\n","\n","  def __init__(self, in_features):\n","    super(MLP, self).__init__()\n","    self.fc1 = nn.Linear(in_features=in_features, out_features=1024)\n","    self.fc2 = nn.Linear(in_features=1024, out_features=512)\n","    self.fc3 = nn.Linear(in_features=512, out_features=256)\n","    self.output = nn.Linear(in_features=256, out_features=2)\n","    self.drop = nn.Dropout(0.4)\n","  \n","  def forward(self, new_input):\n","    out = F.relu(self.fc1(new_input))\n","    out = F.relu(self.fc2(out))\n","    out = F.relu(self.fc3(out))\n","    out = self.output(out)\n","    \n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wl3PabnAX4_1"},"source":["loss_function = nn.CrossEntropyLoss().to(DEVICE)\n","modelo = MLP(300).to(DEVICE)\n","optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)\n","BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCauiKoIX5CJ"},"source":["# Dataloaders\n","train_vectors = [get_sentence_embedding(sent) for sent in train_reviews]\n","test_vectors = [get_sentence_embedding(sent) for sent in test_reviews]\n","\n","train_targets = torch.Tensor(train_sentiments.to_numpy()).long()\n","train_dataset = TensorDataset(torch.Tensor(train_vectors), train_targets) \n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n","\n","test_targets = torch.Tensor(test_sentiments.to_numpy()).long()\n","test_dataset = TensorDataset(torch.Tensor(test_vectors), test_targets) \n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDqPZnJgZWDi","executionInfo":{"status":"ok","timestamp":1602442623519,"user_tz":180,"elapsed":164273,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"1ade3b02-a489-4bf6-bdfa-9eef1a061563","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["train_model(modelo, train_dataloader, test_dataloader, loss_function, optimizer, 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training epoch 1 | Loss 0.410241 | Accuracy 81.14% | Time 5.05 seconds\n","Validation epoch 1 | Loss 0.377579 | Accuracy 84.15% | Time 0.87 seconds\n","Training epoch 2 | Loss 0.368549 | Accuracy 83.90% | Time 4.52 seconds\n","Validation epoch 2 | Loss 0.354880 | Accuracy 85.37% | Time 0.89 seconds\n","Training epoch 3 | Loss 0.358738 | Accuracy 84.31% | Time 4.89 seconds\n","Validation epoch 3 | Loss 0.348519 | Accuracy 85.21% | Time 0.91 seconds\n","Training epoch 4 | Loss 0.352300 | Accuracy 84.57% | Time 4.55 seconds\n","Validation epoch 4 | Loss 0.345719 | Accuracy 85.37% | Time 0.87 seconds\n","Training epoch 5 | Loss 0.347040 | Accuracy 84.85% | Time 4.50 seconds\n","Validation epoch 5 | Loss 0.342859 | Accuracy 85.38% | Time 0.86 seconds\n","Training epoch 6 | Loss 0.342235 | Accuracy 85.10% | Time 4.38 seconds\n","Validation epoch 6 | Loss 0.341208 | Accuracy 85.46% | Time 0.87 seconds\n","Training epoch 7 | Loss 0.337804 | Accuracy 85.31% | Time 4.51 seconds\n","Validation epoch 7 | Loss 0.341508 | Accuracy 85.50% | Time 0.88 seconds\n","Training epoch 8 | Loss 0.332982 | Accuracy 85.59% | Time 4.68 seconds\n","Validation epoch 8 | Loss 0.339235 | Accuracy 85.57% | Time 0.87 seconds\n","Training epoch 9 | Loss 0.328181 | Accuracy 85.80% | Time 4.62 seconds\n","Validation epoch 9 | Loss 0.341435 | Accuracy 85.67% | Time 0.87 seconds\n","Training epoch 10 | Loss 0.323258 | Accuracy 85.99% | Time 4.40 seconds\n","Validation epoch 10 | Loss 0.336607 | Accuracy 85.59% | Time 0.87 seconds\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zBfOKG3iMt9Q"},"source":["# Exploración\n","\n","Exploren otras técnicas de preprocesamiento, tokenizacion, vectorizacion, etc. para ver si puede lograr superar los modelos presentados en clase.\n"]},{"cell_type":"code","metadata":{"id":"T09IUvPVJv06"},"source":[],"execution_count":null,"outputs":[]}]}