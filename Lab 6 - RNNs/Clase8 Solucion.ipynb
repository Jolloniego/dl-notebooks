{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clase8 Solucion.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7VMtkNDwxZ0m"},"source":["# Datos y Preprocesamiento\n","\n","Vamos a usar el dataset de IMDB para clasificación de reseñas de películas, el objetivo del mismo es detectar si una reseña tiene sentimiento **positivo** o **negativo**.\n","\n","Descarguen el dataset de este [link](https://drive.google.com/file/d/1i0bBI4p80AxsLgnWcXkxVT65AahIzePu/view?usp=sharing) y subanlo a una carpeta **data** en la raiz de su drive personal.\n"]},{"cell_type":"code","metadata":{"id":"zh7A9mjs3cfU","executionInfo":{"status":"ok","timestamp":1603402329359,"user_tz":180,"elapsed":74265,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"53565102-21d6-4bb1-dccb-242fe95393e3","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","! cp \"/content/drive/My Drive/data/IMDB_Dataset.zip\" .\n","! unzip -q IMDB_Dataset.zip\n","! rm IMDB_Dataset.zip\n","! ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"," drive\t'IMDB Dataset.csv'   sample_data   word2vec.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sJYwO9zbvCLh","executionInfo":{"status":"ok","timestamp":1603402334643,"user_tz":180,"elapsed":79514,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"6b19d8b6-e926-434f-af21-d460080fd75b","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import re\n","import time\n","from itertools import chain\n","from bs4 import BeautifulSoup\n","from collections import Counter\n","\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BBaFc6ZZzp2T","executionInfo":{"status":"ok","timestamp":1603402335116,"user_tz":180,"elapsed":79979,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"754bd07f-4b19-4281-f02f-20bdc00fc8ad","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["imdb_data = pd.read_csv(\"IMDB Dataset.csv\")\n","\n","#sentiment count\n","print(imdb_data.columns)\n","imdb_data['sentiment'].value_counts()\n","\n","# Convert positive and negative into binary classes (1-0)\n","from sklearn.preprocessing import LabelBinarizer\n","lb = LabelBinarizer()\n","\n","sentiment_data = lb.fit_transform(imdb_data[\"sentiment\"])\n","imdb_data['sentiment'] = sentiment_data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['review', 'sentiment'], dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jKV_HYQCvEFv"},"source":["def strip_html(text):\n","  soup = BeautifulSoup(text, \"html.parser\")\n","  return soup.get_text()\n","\n","\n","def remove_between_square_brackets(text):\n","  return re.sub('\\[[^]]*\\]', '', text)\n","\n","\n","def remove_special_characters(text):\n","  pattern = r'[^a-zA-z\\s]'\n","  text = re.sub(pattern,'',text)\n","  return text\n","\n","\n","def low_level_preproc(text):\n","  text = strip_html(text)\n","  text = remove_between_square_brackets(text)\n","  text = remove_special_characters(text)\n","  return text\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].apply(low_level_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orEVYliBvEmq"},"source":["all_stopwords = set(stopwords.words(\"english\"))\n","\n","def remove_stop_words(full_text_line):\n","  tokens = full_text_line.split()\n","  tokens = [tok for tok in tokens if tok not in all_stopwords]\n","\n","  return \" \".join(tokens)\n","\n","\n","def lemmatize(text):\n","  wnl= WordNetLemmatizer()\n","  lemas = [wnl.lemmatize(word) for word in text.split()]\n","\n","  return \" \".join(lemas)\n","\n","\n","def high_level_preproc(text):\n","  text = remove_stop_words(text)\n","  return lemmatize(text)\n","\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].str.lower()\n","imdb_data['review'] = imdb_data['review'].apply(high_level_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgyBKqd6uhLz","executionInfo":{"status":"ok","timestamp":1603402366486,"user_tz":180,"elapsed":111338,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"1e97f8d4-da69-4b9f-8028-da8cd688c8ef","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#split the dataset  \n","#train dataset\n","train_reviews = imdb_data.review[:40000]\n","train_sentiments = imdb_data.sentiment[:40000]\n","\n","#test dataset\n","test_reviews = imdb_data.review[40000:]\n","test_sentiments = imdb_data.sentiment[40000:]\n","\n","\n","print(\"Train set:\", train_reviews.shape, train_sentiments.shape)\n","print(\"Test set:\", test_reviews.shape, test_sentiments.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train set: (40000,) (40000,)\n","Test set: (10000,) (10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6QZ6OiUBrzbM"},"source":["# Vocabulario y Encoding\n","\n","Vamos a crear un volcabulario para el problema, de este modo podemos representar cada palabra con un entero único. Esto nos va a permitir representar una review como una lista de ints (que luego el modelo va a mapear a word embeddings!).\n","\n","Una cosa a tener en cuenta es que vamos a querer agregar padding a nuestros inputs (para que todas las reviews tengan el mismo largo), para esto vamos a usar el 0, por lo que las palabras de nuestro vocabulario deben empezar en 1.\n","\n"]},{"cell_type":"code","metadata":{"id":"Nu2ZDb1Ury0b"},"source":["def make_vocab(all_texts, max_vocab_size, oov_token=\"<OOV>\"):\n","  # Count the number of occurrences of each word\n","  counts = Counter(chain(*(all_texts.str.split())))\n","\n","  # Create vocab containing max_vocab_size tokens\n","  vocab = sorted(counts, key=counts.get, reverse=True)[:max_vocab_size]\n","  vocab.append(oov_token) # Add the out of vocabulary at the end\n","\n","  # Map from word to int index in vocab\n","  vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)} \n","\n","  return vocab_to_int"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ALBSYzBx6ag"},"source":["vocab_mapping = make_vocab(train_reviews, 200_000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnQOZt1PyNYW"},"source":["Ahora vamos a implementar una funcion que transforma un string con la review en una lista de enteros con la posiscion de cada una de nuestras palabras en el vocabulario. Si una palabra no está en el vocabulario usamos el indice para`\"<OOV>\"`"]},{"cell_type":"code","metadata":{"id":"XfoHxQwExN8W"},"source":["def get_review_features(text, word_to_idx):\n","  indices = [word_to_idx[word] if word in word_to_idx else word_to_idx[\"<OOV>\"] for word in text.split()]\n","  \n","  return indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GAOv3JUm1g2x"},"source":["juan = get_review_features(train_reviews[0], vocab_mapping)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mQZ0dRi0A3t"},"source":["Lo siguiente es implementar padding de las sentencias, si bien los modelos son capaces de trabajar con secuencias de cualquier largo queremos que el tiempo de entrenamiento e inferencia esté controlado y no dependa del largo de los inputs. Como una pequeña optimizacion vamos a hacer **left padding**, es decir, agregar 0s a la izquierda de una secuencia hasta alcanzar `max_sequence_length` elementos.\n","\n","Agregar ceros a la izquierda ayuda a los modelos a aprender de los datos ya que la informacion valiosa aparece al final de la secuencia y no tiene que recordar 3 palabras en luego de haber visto 100 ceros..\n","\n","Ejemplo, la secuencia\n","\n","`[117, 18, 128]`\n","\n"," Quedaría:\n","\n","`[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]`\n","\n","En lugar de \n","\n","`[117, 18, 128, 0, 0, 0, 0, 0, 0, 0] ` Forzando al modelo a recordar los 3 primeros inputs para poder predecir algo.\n"]},{"cell_type":"code","metadata":{"id":"8DY_mllOqOvX"},"source":["def pad_features(review_ints, sequence_length):\n","  features = np.zeros(sequence_length, dtype=np.int32)\n","  features[-len(review_ints):] = np.array(review_ints)[:sequence_length]\n","\n","  return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pz43OMIV2ejT"},"source":["def get_review_representation(review_text, word_to_idx, max_sequence_length):\n","  return pad_features(get_review_features(review_text, word_to_idx), max_sequence_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJ99DiItmGq7","executionInfo":{"status":"ok","timestamp":1603408548841,"user_tz":180,"elapsed":545,"user":{"displayName":"Juan Olloniego","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFZMy1xYdrq8EuW2y-CLv6Eae8Tfe4Qp-uZpYfQQ=s64","userId":"13959738928832749591"}},"outputId":"ec64443b-8149-40dd-aa3f-89a0b1c20063","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","\n","np.array([1,2,3,4,5,6,7,8,9])[-3:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7, 8, 9])"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"lXrRSrbX2AqF"},"source":["# Transformando los textos a vectores"]},{"cell_type":"code","metadata":{"id":"JaSoT9Vu2RAX"},"source":["MAX_SEQUENCE_LENGTH = 100\n","\n","train_vectors = train_reviews.apply(lambda x: get_review_representation(x, vocab_mapping, MAX_SEQUENCE_LENGTH))\n","test_vectors = test_reviews.apply(lambda x: get_review_representation(x, vocab_mapping, MAX_SEQUENCE_LENGTH))\n","\n","train_vectors = np.array([vec for vec in train_vectors])\n","test_vectors = np.array([vec for vec in test_vectors])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6uOvDvL43-L"},"source":["# Codigo de entrenamiento e inferencia \n","\n","Same old..\n"]},{"cell_type":"code","metadata":{"id":"Bc8LaIv149Hg"},"source":["def train_epoch(training_model, loader, criterion, optim):\n","    training_model.train()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    for data, labels in loader:\n","      all_labels.extend(labels.numpy())  \n","\n","      optim.zero_grad()\n","\n","      predictions = training_model(data.to(DEVICE))\n","      all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","      loss = criterion(predictions, labels.to(DEVICE))\n","      \n","      loss.backward()\n","      optim.step()\n","\n","      epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","\n","\n","def validation_epoch(val_model, loader, criterion):\n","    val_model.eval()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    with torch.no_grad():\n","      for data, labels in loader:\n","        all_labels.extend(labels.numpy())  \n","\n","        predictions = val_model(data.to(DEVICE))\n","        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","        loss = criterion(predictions, labels.to(DEVICE))\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","  \n","\n","def train_model(model, train_loader, test_loader, criterion, optim, number_epochs):\n","  train_history = []\n","  test_history = []\n","  accuracy_history = []\n","\n","  for epoch in range(number_epochs):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n","      train_history.append(train_loss)\n","      print(\"Training epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, train_loss, train_acc, time.time() - start_time))\n","\n","      start_time = time.time()\n","      test_loss, acc = validation_epoch(model, test_loader, criterion)\n","      test_history.append(test_loss)\n","      accuracy_history.append(acc)\n","      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, test_loss, acc, time.time() - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkFS0KDj5AKp"},"source":["# Modelo\n"]},{"cell_type":"code","metadata":{"id":"VmHNNvSfHLq_"},"source":["class SentimentRNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(SentimentRNN, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n","        self.drop = nn.Dropout(0.3)\n","        self.fc1 = nn.Linear(hidden_dim, 128)\n","        self.out = nn.Linear(128, 2)\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)  \n","        rnn_out, _ = self.rnn(embeds)\n","        \n","        out = self.drop(rnn_out[:, -1])\n","        out = F.relu(self.fc1(out))\n","        out = self.drop(out)\n","        \n","        out = self.out(out)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssVUQ3CL-V6-"},"source":["modelo = SentimentRNN(vocab_size=len(vocab_mapping) + 1, embedding_dim=300, hidden_dim=512).to(DEVICE)\n","optimizer = torch.optim.Adam(modelo.parameters(), lr=0.00005)\n","train_model(modelo, train_dataloader, test_dataloader, loss_function, optimizer, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HmEfoQW5S8t"},"source":["class SentimentLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(SentimentLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.drop = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(hidden_dim, 128)\n","        self.out = nn.Linear(128, 2)\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        embeds = self.drop(embeds)\n","        lstm_out, (ht, ct) = self.lstm(embeds)\n","\n","        out = self.drop(ht.squeeze())\n","        out = F.relu(self.fc1(out))\n","        out = self.drop(out)\n","        \n","        out = self.out(out)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQ8gjZ86HYDf"},"source":["modelo = SentimentLSTM(vocab_size=len(vocab_mapping) + 1, embedding_dim=300, hidden_dim=512).to(DEVICE)\n","optimizer = torch.optim.Adam(modelo.parameters(), lr=0.002)\n","train_model(modelo, train_dataloader, test_dataloader, loss_function, optimizer, 15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sahj6WEl5Tii"},"source":["# Entrenamiento"]},{"cell_type":"code","metadata":{"id":"JqW4_B46HZTt"},"source":["loss_function = nn.CrossEntropyLoss().to(DEVICE)\n","BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iw76sUmhyxAw"},"source":["train_targets = torch.Tensor(train_sentiments.to_numpy()).long()\n","train_dataset = TensorDataset(torch.LongTensor(train_vectors), train_targets) \n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n","\n","test_targets = torch.Tensor(test_sentiments.to_numpy()).long()\n","test_dataset = TensorDataset(torch.LongTensor(test_vectors), test_targets) \n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-Dw-KmYDAjF"},"source":["# Mejoras en el modelo\n","\n","\n","\n","1. Podemos mejorar la performance si usamos una GRU o una LSTM ?\n","2. Que pasa si usamos celdas **bidireccionales** ?\n","3. Que pasa si aumentamos el numero de **capas** de nuestras celdas recurrentes \n","?\n","4. Y si usamos vectores preentrenados (W2V, GloVe) ?\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"6e7iF06_Dcdk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rtsn-YNxDcgB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veuzMIyjDchz"},"source":[""],"execution_count":null,"outputs":[]}]}